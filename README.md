# Bachelorarbeit â€“ Pipeline: Automatisierte Highlight-Erkennung & 9:16-Aufbereitung

Diese Repository enthÃ¤lt eine vollstÃ¤ndige, skriptbasierte Pipeline, um aus Langvideos automatisch Socialâ€‘Mediaâ€‘taugliche 9:16â€‘Highlights zu erzeugen â€“ inkl. Transkription, KIâ€‘gestÃ¼tzter Clipâ€‘Selektion, Gesichtsâ€‘/MundaktivitÃ¤tsanalyse, Autoâ€‘Cropping, Untertitel (Wordâ€‘Caps) und finalem Export.

## Inhaltsverzeichnis
- [Features](#features)
- [Ordnerstruktur](#ordnerstruktur)
- [Voraussetzungen](#voraussetzungen)
- [Installation](#installation)
- [Schnellstart (empfohlener Workflow)](#schnellstart-empfohlener-workflow)
- [Skripte & CLI](#skripte--cli)
- [Tipps & Troubleshooting](#tipps--troubleshooting)
- [Reproduzierbarkeit](#reproduzierbarkeit)
- [Lizenz / Danksagung](#lizenz--danksagung)

---

## Features
- **Transkription mit Wortâ€‘Zeitstempeln (Whisper, chunked ohne Grenzâ€‘Doppler)**
- **LLMâ€‘gestÃ¼tzte Clipâ€‘Selektion** (ViralitÃ¤t/EmotionalitÃ¤t etc. in SQLite gespeichert)
- **Faceâ€‘Detection (YOLOv8â€‘face) & MundaktivitÃ¤t (MediaPipe)**
- **Stabiles 9:16â€‘Autoâ€‘Cropping** (Median + EMA, Deadband, Szenenschnittâ€‘Erkennung, Switchâ€‘Cooldown)
- **Wordâ€‘Caps Untertitel** (ASS generiert, per ffmpeg eingebrannt)
- **Batchâ€‘Export der Highlights** (MoviePy, LÃ¤ngenâ€‘/Grenzâ€‘Checks)

## Ordnerstruktur
Die Pfade werden zentral in `config.py` definiert:
```
PROJECT_ROOT/
â”œâ”€ data/
â”‚  â”œâ”€ input/                 # Eingabevideo(s)
â”‚  â”œâ”€ transkripte/           # Whisper-Outputs (*_segments.json, *_timed.txt ...)
â”‚  â”œâ”€ segments/              # LLM-Clip-Auswahl, DB etc.
â”‚  â”œâ”€ output/
â”‚  â”‚  â””â”€ raw_clips/          # Roh-Highlight-Clips (aus cutClips.py)
â”‚  â”œâ”€ face_data_combined/    # faces.json je Clip (YOLO + MediaPipe)
â”‚  â””â”€ face_crop_centers/     # (optional) Center-Listen
â”œâ”€ output/
â”‚  â”œâ”€ output_9x16_final/         # Auto-cropped 9:16 Videos
â”‚  â”œâ”€ output_9x16_final_subbed_word/  # 9:16 mit eingebrannten Word-Caps
â”‚  â””â”€ debug/                     # Debug-Previews/Artefakte
â”œâ”€ models/                    # YOLO-Weights (z. B. yolov8n-face.pt)
â”œâ”€ whisper-cache/            # Whisper Modell-Cache
â””â”€ src/... (optional projektspezifisch)
```
> Beim ersten Start legt `config.py` fehlende Verzeichnisse automatisch an.

## Voraussetzungen
**Systemâ€‘Tools**
- `ffmpeg` (inkl. `ffprobe`) im `PATH`

**Python**
- Python 3.10+ empfohlen
- Pakete (Beispiel):  
  `openai-whisper`, `torch`, `ffmpeg-python`, `ultralytics`, `opencv-python`, `mediapipe`, `moviepy`, `tqdm`, `numpy`, `regex`
- Optional/abhÃ¤ngig vom Codepfad: `pydub`, `scikit-image` (falls in Erweiterungen verwendet)

**Modelle & Keys**
- **Whisper**: lÃ¤dt Modelle automatisch in `whisper-cache/` (steuerbar via `WHISPER_MODEL`)
- **YOLOv8â€‘face**: `models/yolov8n-face.pt` (oder grÃ¶ÃŸeres Modell)
- **OpenAI API Key** (fÃ¼r `segment_transcript.py` & `rateCluster.py`): `export OPENAI_API_KEY=...`
  - Defaultâ€‘Modell ggf. per `export OPENAI_MODEL=gpt-4o` setzen

## Installation
```bash
# 1) Python-Umgebung
python3 -m venv .venv
source .venv/bin/activate

# 2) SystemabhÃ¤ngigkeiten
# ffmpeg installieren (Mac: brew install ffmpeg, Ubuntu: apt install ffmpeg)

# 3) Python-Pakete (Beispiel)
pip install --upgrade pip
pip install openai-whisper torch ffmpeg-python ultralytics opencv-python mediapipe moviepy tqdm numpy regex

# 4) Modelle/Dateien
# YOLO-Weights:
#   Download yolov8n-face.pt â†’ ./models/yolov8n-face.pt
# API Key fÃ¼r LLM:
export OPENAI_API_KEY="sk-..."
export OPENAI_MODEL="gpt-4o"
```

## Schnellstart (empfohlener Workflow)
1) **Eingabe ablegen**  
   Lege dein Langvideo in `data/input/` (z.â€¯B. `meinvideo.mp4`).

2) **Transkription (Whisper, chunked & doppler-sicher)**  
```bash
python transcription.py --input data/input/meinvideo.mp4 --model small --lang de
```
   â†’ erzeugt `*_segments.json` + `*_timed.txt` in `data/transkripte/`.

3) **Clips mit LLM selektieren & in DB speichern**  
```bash
export OPENAI_API_KEY="..."; export OPENAI_MODEL="gpt-4o"
python segment_transcript.py --base meinvideo --block 60 --min 6.0 --max 30.0
```
   â†’ schreibt Clips in SQLite (`data/clips_openai.db` o. Ã¤.)

4) **Highlights aus dem Originalvideo schneiden**  
```bash
python cutClips.py --file meinvideo.mp4 --limit 10 --order score
```
   â†’ exportiert `highlight_*.mp4` nach `data/output/raw_clips/`

5) **Faceâ€‘Detection + MundaktivitÃ¤t**  
```bash
python main_detect_faces.py --model models/yolov8n-face.pt     --input-dir data/output/raw_clips --output-dir data/face_data_combined     --frame-skip 1 --downscale 0.5
```

6) **Targets je Frame bauen (Zentren/GrÃ¶ÃŸe glÃ¤tten)**  
```bash
python make_segments.py --pattern "highlight_*.mp4" --fps 0 --smooth 7 --overwrite
```

7) **9:16 Autoâ€‘Crop anwenden**  
```bash
python main_apply_crop.py --pattern "highlight_*.mp4" --median 7 --ema 0.5     --deadband 16 --cut_detect --mux_audio --overwrite
```
   â†’ fertige 9:16â€‘Clips in `output/output_9x16_final/`

8) **Wordâ€‘Caps Untertitel einbrennen (optional)**  
```bash
python add_subtitles.py --clips_dir output/output_9x16_final     --out_dir output/output_9x16_final_subbed_word --model small --limit 20
```
   â†’ fertige Videos mit eingebrannten Wordâ€‘Caps in `output/output_9x16_final_subbed_word/`

> ðŸ’¡ Du kannst viele Parameter (Fensterbreiten, Deadband, Erkennungsschwellen, Limits) Ã¼ber die CLI anpassen.

## Skripte & CLI
### `transcription.py`
Chunkedâ€‘Transkription mit Wortzeitstempeln.
```
--input PATH        # Eingabevideo/-audio (Default: erstes File in data/input/)
--outdir PATH       # Ausgabeverzeichnis (Default: data/transkripte/)
--model NAME        # Whisper-Modell (tiny/base/small/medium/large; env: WHISPER_MODEL)
--lang CODE         # Sprachcode (z.â€¯B. de) oder leer/None fÃ¼r Auto-Detect
--chunk FLOAT       # Chunk-LÃ¤nge in s (Default 60)
--overlap FLOAT     # Ãœberlappung in s (Default 2.0)
--min-dur FLOAT     # Mindest-Segmentdauer (s)
--max-gap FLOAT     # Max. Zeit-Gap beim Mergen (s)
--fp16              # Nur sinnvoll mit GPU
```

### `segment_transcript.py`
LLMâ€‘Selektion & Speichern in SQLite.
```
--base STR          # Basename der Transkriptdateien (z.â€¯B. 'meinvideo')
--block FLOAT       # BlocklÃ¤nge s fÃ¼r den Prompt
--min FLOAT         # minimale Clip-LÃ¤nge s
--max FLOAT         # maximale Clip-LÃ¤nge s
# env: OPENAI_API_KEY, OPENAI_MODEL (z. B. gpt-4o)
```

### `cutClips.py`
Schneidet ausgewÃ¤hlte Highlights als Einzelclips.
```
--file NAME         # Name der Input-Datei in data/input (Default: erstes Video)
--limit INT         # Anzahl zu exportierender Clips (Default 10)
--order {score,start} # Sortierung: Score (absteigend) oder Startzeit
```

### `main_detect_faces.py`
YOLOv8â€‘face + MediaPipe â†’ `faces.json` pro Clip.
```
--input-dir PATH    # Default: data/output/raw_clips
--output-dir PATH   # Default: data/face_data_combined
--model PATH        # YOLOv8-face Weights (Default: models/yolov8n-face.pt)
--conf-thresh FLOAT # Default 0.35
--frame-skip INT    # z. B. 1 = jeden Frame, 2 = jeden von zwei ...
--downscale FLOAT   # Frame-Downscale vor YOLO (0..1, z. B. 0.5)
--expansion FLOAT   # Margin Pass 1 (relativ)
--expansion2 FLOAT  # Margin Pass 2 (relativ)
--min-crop INT      # minimale CroplÃ¤nge (px)
--faces-upscale INT # min. KantenlÃ¤nge fÃ¼r FaceMesh (kleine Crops hochskalieren)
--imgsz INT         # YOLO input size (Default 448)
--max-det INT       # Max Detects / Frame
--use-refine        # MediaPipe refine_landmarks aktivieren
```

### `make_segments.py`
Erzeugt `*_target_by_frame.json` (Zentren+Side pro Frame) aus Face/Centerâ€‘Daten.
```
--pattern STR       # Dateimuster in raw_clips (Default: highlight_*.mp4)
--fps FLOAT         # FPS erzwingen (0 = aus Video lesen)
--smooth INT        # MA-Fensterbreite (ungerade)
--overwrite         # bestehende target_by_frame.json Ã¼berschreiben
```

### `main_apply_crop.py`
Wendet 9:16â€‘Crop mit GlÃ¤ttung/Szenenschnitt an.
```
--pattern STR       # Dateimuster in raw_clips (Default: *.mp4)
--out_w INT         # Output-Breite (Default 1080)
--out_h INT         # Output-HÃ¶he (Default 1920)
--zoom_pad FLOAT    # Zoom-Pad (0..1)
--median INT        # Median-Fenster (>=3, ungerade)
--ema FLOAT         # EMA-Alpha (0..1)
--deadband FLOAT    # Totband in Pixel
--switch_cd INT     # Cooldown-Frames nach Trackwechsel
--cut_detect        # Szenenschnitt-Erkennung aktivieren
--cut_corr FLOAT    # Schwellwert Korrelation (0..1)
--cut_cd INT        # Cooldown-Frames nach Cut
--mux_audio         # Original-Audio unterlegen
--debug             # Debug-Overlay anzeigen
--debug_scale FLOAT # Debug-Preview skaliert rendern
--overwrite         # vorhandene Ausgaben Ã¼berschreiben
```

### `add_subtitles.py`
Generiert Wordâ€‘Caps mit Whisper & brennt sie ein.
```
--clips_dir PATH    # Quelle (Default: output/output_9x16_final)
--out_dir PATH      # Ziel   (Default: output/output_9x16_final_subbed_word)
--pattern STR       # z. B. *.mp4
--limit INT         # Nur die ersten N Clips
--model NAME        # Whisper-Modell (tiny/base/small/medium/large)
--lang CODE         # Sprachcode oder Auto
```

### `rateCluster.py` (optional)
LÃ¤sst LLM Scores (ViralitÃ¤t, Emotion, Humor, Provokation) nachtragen.
> Modelliere Standardâ€‘Modell via `OPENAI_MODEL` (z.â€¯B. `gpt-4o`).

---

## Tipps & Troubleshooting
- **Modelle/Performance**
  - CPUâ€‘only ist mÃ¶glich (Whisper/YOLO langsamer). Auf Apple Silicon wird automatisch **MPS** genutzt; auf NVIDIA **CUDA**.
  - `--frame-skip` und `--downscale` in `main_detect_faces.py` beschleunigen die Faceâ€‘Detection deutlich.
- **ffmpegâ€‘Muxing prÃ¼fen** (`main_apply_crop.py --mux_audio`): Falls Ton fehlt, `ffmpeg`-Installation checken. RÃ¼ckgabecode im Log prÃ¼fen.
- **Fehlende Dateien**
  - Kein Input? â†’ `data/input/` prÃ¼fen.
  - Fehlende Transkriptâ€‘Paare? â†’ `*_timed.txt` und `*_segments.json` mÃ¼ssen existieren (aus `transcription.py`).
  - Fehlende Faces? â†’ Pfad zu `models/yolov8n-face.pt` korrekt?
- **Datenbank**
  - Highlights liegen in SQLite (siehe `config.py`: `DB_PATH`). Bei Wiederholungen kann ein `DELETE FROM highlights; VACUUM;` sinnvoll sein.
- **Cache/Verzeichnisse**
  - Whisperâ€‘Cache via `XDG_CACHE_HOME` â†’ `whisper-cache/` neben dem Projekt. Speicherplatz beachten.

## Reproduzierbarkeit
- Lege eine `requirements.txt` mit exakten Versionen an (Freeze deiner funktionierenden Umgebung).
- Dokumentiere verwendete **Modellâ€‘VersionsstÃ¤nde** (YOLO Weights, Whisperâ€‘ModellgrÃ¶ÃŸe, OPENAI_MODEL).
- Fixiere Randomâ€‘Seeds, falls nÃ¶tig (hier meist deterministisch durch externe Modelle/Bibliotheken).

## Lizenz / Danksagung
- Verwendung von **OpenAI Whisper**, **Ultralytics YOLOv8**, **MediaPipe**, **OpenCV**, **MoviePy**, **ffmpeg**.
- Die jeweiligen Lizenzen der Bibliotheken beachten.
